import sys
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import pandas as pd
import json
import os
import subprocess
from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="Signalizer 3.5 API")

from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Models ---
class MatchSignal(BaseModel):
    League: str
    Date: str
    Home: str
    Away: str
    Odds: float
    Confidence: str

class ScanRequest(BaseModel):
    days: int = 7

class AnalyzeRequest(BaseModel):
    matches: List[str]
    model: str = "gpt-3.5-turbo" # Default

class KellyRequest(BaseModel):
    odds: float
    win_prob: float
    bankroll: float

# ... (rest of code)

# --- Helpers ---
def translate_teams_batch(teams: List[str]) -> dict:
    """Translates a list of team names to Russian (Winline style) using LLM."""
    if not teams: return {}
    
    try:
        from openai import OpenAI
        api_key = os.getenv("OPENAI_API_KEY")
        client = OpenAI(api_key=api_key)
        
        teams_txt = "\n".join(teams)
        prompt = f"""
        ÐŸÐµÑ€ÐµÐ²ÐµÐ´Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÑÑ‚Ð¸Ñ… Ñ„ÑƒÑ‚Ð±Ð¾Ð»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð½Ð° Ð Ð£Ð¡Ð¡ÐšÐ˜Ð™ ÑÐ·Ñ‹Ðº.
        Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ, Ð¿Ñ€Ð¸Ð½ÑÑ‚ÑƒÑŽ Ð² Ð‘Ðš Winline.
        
        Ð’ÐÐ–ÐÐž:
        1. ÐÐ• ÐžÐ¡Ð¢ÐÐ’Ð›Ð¯Ð™ ÐÐÐ“Ð›Ð˜Ð™Ð¡ÐšÐ˜Ð¥ ÐÐÐ—Ð’ÐÐÐ˜Ð™. Ð’ÑÐµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ ÐºÐ¸Ñ€Ð¸Ð»Ð»Ð¸Ñ†ÐµÐ¹.
        2. Ð•ÑÐ»Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ñ€ÐµÐ´ÐºÐ¾Ðµ - Ð¿Ñ€Ð¾ÑÑ‚Ð¾ Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ‚ÐµÑ€Ð¸Ñ€ÑƒÐ¹ (Racing -> Ð Ð°ÑÐ¸Ð½Ð³).
        
        Ð¡ÐŸÐ˜Ð¡ÐžÐš:
        {teams_txt}
        
        Ð¤ÐžÐ ÐœÐÐ¢ ÐžÐ¢Ð’Ð•Ð¢Ð (JSON):
        {{
            "Original Name": "Russian Name",
            ...
        }}
        Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON.
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a translator helper. Output strictly JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        import json
        return json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"Translation Error: {e}")
        return {t: t for t in teams} # Fallback to original

def load_signals():
    csv_file = "under35_signals_5leagues.csv"
    ru_csv_file = "under35_signals_5leagues_ru.csv"
    
    try:
        if not os.path.exists(csv_file):
            return []
            
        # Check Cache Validity
        use_cache = False
        if os.path.exists(ru_csv_file):
            # If RU file is newer than or same age as Source
            if os.path.getmtime(ru_csv_file) >= os.path.getmtime(csv_file):
                use_cache = True
        
        if use_cache:
            try:
                return pd.read_csv(ru_csv_file).to_dict(orient="records")
            except:
                pass # Cache corrupted? Proceed to regenerate
        
        # --- GENERATE TRANSLATION ---
        df = pd.read_csv(csv_file)
        if df.empty: return []
        
        # 1. Get Unique Teams
        teams = pd.concat([df['Home'], df['Away']]).unique().tolist()
        
        # 2. Translate
        print(f"Translating {len(teams)} teams for signals cache...")
        trans_map = translate_teams_batch(teams)
        
        # 3. Apply
        # Handle JSON returning slightly different keys or missing keys gracefully
        # Normalize map slightly?
        
        df['Home'] = df['Home'].map(lambda x: trans_map.get(x, x))
        df['Away'] = df['Away'].map(lambda x: trans_map.get(x, x))
        
        # 4. Save Cache
        df.to_csv(ru_csv_file, index=False)
        
        return df.to_dict(orient="records")
        
    except Exception as e:
        print(f"Load Signals Error: {e}")
        return []

# --- Endpoints ---

@app.get("/")
def read_root():
    return {"status": "active", "system": "Signalizer 3.5"}

@app.post("/scan/{days}")
def run_scan(days: int):
    """Trigger the external scanner script"""
    try:
        # Calling the script as a subprocess ensures isolation
        result = subprocess.run(
            [sys.executable, "under35_scanner.py"], 
            capture_output=True, text=True
        )
        if result.returncode == 0:
            signals = load_signals()
            return {"status": "success", "found": len(signals), "log": result.stdout}
        else:
            raise HTTPException(status_code=500, detail=result.stderr)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/signals")
def get_signals():
    """Get current signals"""
    return load_signals()

@app.get("/backtest")
def get_backtest():
    """Mock backtest data"""
    return {
        "Primeira Liga": {"ROI": "6.2%", "WinRate": "68%"},
        "Greek Super League": {"ROI": "4.1%", "WinRate": "62%"},
        "La Liga 2": {"ROI": "5.5%", "WinRate": "65%"},
        "Eredivisie": {"ROI": "3.8%", "WinRate": "60%"},
        "Argentina Liga": {"ROI": "7.0%", "WinRate": "72%"}
    }

@app.post("/analyze_express")
def analyze_express(req: AnalyzeRequest):
    """
    AI Analysis using OpenAI/Perplexity.
    Uses keys from .env
    """
    matches_text = "\n".join(req.matches)
    
    # 1. Check Cache
    try:
        current_key = sorted(req.matches)
        history = get_ai_cache()
        for item in history:
            # Check cached items
            if item.get("matches_key") == current_key:
                # Found valid cache!
                return {
                    "analysis": item["analysis"],
                    "recommendation": f"Loaded from Cache ({item['date_str']})",
                    "cached": True,
                    "timestamp": item.get("timestamp")
                }
    except Exception as e:
        print(f"Cache Check Error: {e}")
    
    # Enhanced prompt in Russian
    prompt = f"""
ðŸ¤– ÐÐÐÐ›Ð˜Ð— ÐœÐÐ¢Ð§Ð•Ð™ (Ð¢Ðœ 3.5)

ðŸ“‹ ÐœÐÐ¢Ð§Ð˜:
{matches_text}

ðŸ“Š Ð—ÐÐ”ÐÐ§Ð: Ð”Ð»Ñ ÐšÐÐ–Ð”ÐžÐ“Ðž Ð¼Ð°Ñ‚Ñ‡Ð° Ð´Ð°Ð¹ Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð· Ñ ÑÐ¼Ð¾Ð´Ð·Ð¸:
1. ðŸ“… Ð”ÐÐ¢Ð Ð¸ Ð’Ñ€ÐµÐ¼Ñ (ÐœÐ¡Ðš).
2. Ð¢Ð Ð˜ (3) Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… ÑÑ‡ÐµÑ‚Ð°.
3. Ð’ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÑŒ Ð¢Ðœ 3.5.
4. Ð£Ð²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ.
5. ÐŸÑ€Ð¸Ñ‡Ð¸Ð½Ð° (ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ ÑÑ†ÐµÐ½Ð°Ñ€Ð¸Ð¹ Ð¼Ð°Ñ‚Ñ‡Ð°).

âš ï¸ Ð’ÐÐ–ÐÐž: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ¾Ð¼Ð°Ð½Ð´ Ð¿Ð¸ÑˆÐ¸ Ð¡Ð¢Ð ÐžÐ“Ðž ÐÐ Ð Ð£Ð¡Ð¡ÐšÐžÐœ ÑÐ·Ñ‹ÐºÐµ (Ñ‚Ñ€Ð°Ð½ÑÐ»Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ñ Winline).
ÐŸÑ€Ð¸Ð¼ÐµÑ€: "Racing Club" -> "Ð Ð°ÑÐ¸Ð½Ð³ ÐšÐ»Ð°Ð±".

ðŸ“ˆ Ð¤ÐžÐ ÐœÐÐ¢ (Ð¡Ñ‚Ñ€Ð¾Ð³Ð¾, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÑ‹ ÑÑ‚Ñ€Ð¾Ðº!):
âš½ Ð Ð°ÑÐ¸Ð½Ð³ ÐšÐ»Ð°Ð± vs Ð‘Ð¾ÐºÐ° Ð¥ÑƒÐ½Ð¸Ð¾Ñ€Ñ (ÐŸÐ•Ð Ð•Ð’Ð•Ð”Ð˜ ÐÐÐ—Ð’ÐÐÐ˜Ð¯!)
ðŸ“… Ð”ÐÐ¢Ð: 23.10 19:00 ÐœÐ¡Ðš

ðŸŽ¯ Ð¡Ð§Ð•Ð¢Ð:
ðŸ’Ž 1:1 (40%)
ðŸ”¹ 2:0 (30%)
ðŸ”¹ 3:0 (15%)

ðŸ“‰ Ð¢Ðœ 3.5: 82%
ðŸ›¡ï¸ Ð£Ð’Ð•Ð Ð•ÐÐÐžÐ¡Ð¢Ð¬: 8/10
ðŸ“ ÐŸÐ Ð˜Ð§Ð˜ÐÐ: Ð“Ð¾ÑÑ‚Ð¸ Ñ„Ð°Ð²Ð¾Ñ€Ð¸Ñ‚Ñ‹, Ñ…Ð¾Ð·ÑÐµÐ²Ð° Ð¼Ð°Ð»Ð¾ Ð·Ð°Ð±Ð¸Ð²Ð°ÑŽÑ‚, Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ Ð²ÑÐ·ÐºÐ°Ñ Ð¸Ð³Ñ€Ð°.

âš ï¸ Ð’ÐÐ–ÐÐž:
- Ð£Ð”ÐÐ›Ð¯Ð™ Ð»ÑŽÐ±Ñ‹Ðµ ÑÑÑ‹Ð»ÐºÐ¸ Ð½Ð° Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ñ‚Ð¸Ð¿Ð° [1][2].
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¿ÑƒÑÑ‚ÑƒÑŽ ÑÑ‚Ñ€Ð¾ÐºÑƒ Ð¼ÐµÐ¶Ð´Ñƒ Ð¼Ð°Ñ‚Ñ‡Ð°Ð¼Ð¸.
- ÐŸÑ€Ð¸Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°Ð¹ÑÑ ÑÑ‚Ð¸Ð»Ñ Ñ ÑÐ¼Ð¾Ð´Ð·Ð¸.
    """
    
    try:
        from openai import OpenAI
        
        # Select provider based on model choice
        if "Perplexity" in req.model or "Sonar" in req.model:
            api_key = os.getenv("PERPLEXITY_API_KEY")
            base_url = "https://api.perplexity.ai"
            model_id = "sonar" # Defaulting to 'sonar' (prev. sonar-medium-online or similar)
        else:
            api_key = os.getenv("OPENAI_API_KEY")
            base_url = None
            model_id = "gpt-4o-mini" # User requested this as backup
            
        client = OpenAI(api_key=api_key, base_url=base_url)
        
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "system", "content": "Ð¢Ñ‹ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº, ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¹ÑÑ Ð½Ð° Ñ„ÑƒÑ‚Ð±Ð¾Ð»ÑŒÐ½Ð¾Ð¹ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐµ."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1 # Deterministic
        )
        
        analysis = response.choices[0].message.content
        
        # Save to AI Caching History
        try:
            timestamp = datetime.now().timestamp()
            cache_item = {
                "matches": req.matches,
                "matches_key": sorted(req.matches),
                "model": model_id,
                "analysis": analysis,
                "timestamp": timestamp,
                "date_str": datetime.now().strftime("%Y-%m-%d %H:%M")
            }
            save_ai_cache(cache_item)
        except Exception as ex:
            print(f"Cache Save Error: {ex}")

        return {
            "analysis": analysis,
            "recommendation": "Analysis generated by " + model_id,
            "cached": False
        }
    except Exception as e:
        return {
            "analysis": f"AI Analysis Error: {str(e)}",
            "recommendation": "Please check API keys."
        }

AI_HISTORY_FILE = "data/ai_history.json"

def get_ai_cache():
    if os.path.exists(AI_HISTORY_FILE):
        with open(AI_HISTORY_FILE, "r") as f:
            return json.load(f)
    return []

def save_ai_cache(item):
    history = get_ai_cache()
    # Check if duplicate (same matches key and very recent? or just append)
    # We append but maybe limit size?
    history.insert(0, item) # Newest first
    if len(history) > 50: history = history[:50] # Keep last 50
    
    with open(AI_HISTORY_FILE, "w") as f:
        json.dump(history, f, indent=4)

@app.get("/get_ai_history")
def get_ai_history_endpoint():
    return get_ai_cache()

@app.post("/delete_ai_history")
def delete_ai_history(req: DeleteHistoryRequest):
    """
    Delete AI history item by timestamp or delete all.
    """
    try:
        if not os.path.exists(AI_HISTORY_FILE):
             return {"status": "empty"}
             
        with open(AI_HISTORY_FILE, "r") as f:
            history = json.load(f)
            
        initial_len = len(history)
        
        if req.delete_all:
            history = []
        elif req.timestamp:
            # Filter out the item with matching timestamp
            # Use a small tolerance for float comparison if needed, or exact match
            history = [h for h in history if abs(h.get("timestamp", 0) - req.timestamp) > 0.001]
            
        with open(AI_HISTORY_FILE, "w") as f:
            json.dump(history, f, indent=4)
            
        return {"status": "deleted", "deleted_count": initial_len - len(history)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/kelly")
def calculate_kelly(req: KellyRequest):
    """
    f* = (bp - q) / b
    b = odds - 1
    p = win_prob
    q = 1 - p
    """
    b = req.odds - 1
    p = req.win_prob
    q = 1 - p
    
    f_star = (b * p - q) / b
    
    if f_star < 0:
        return {"action": "Do not bet", "fraction": 0, "amount": 0}
    
    amount = req.bankroll * f_star
    return {
        "action": "Bet",
        "fraction": round(f_star, 4),
        "amount": round(amount, 2)
    }

class NotifyRequest(BaseModel):
    message: str

# ... existing imports ...
from datetime import datetime

# History Data Model
class HistoryItem(BaseModel):
    date: str
    matches: list[str]
    outcomes: dict  # {m1: [o1,o2,o3], ...}
    odds: dict # {m1: [1.5, ...], ...}
    variations_count: int
    roi_calculation: str
    timestamp: float

HISTORY_FILE = "data/history.json"

@app.post("/save_history")
def save_history(item: HistoryItem):
    try:
        history = []
        if os.path.exists(HISTORY_FILE):
             with open(HISTORY_FILE, "r") as f:
                 history = json.load(f)
        
        history.append(item.dict())
        
        with open(HISTORY_FILE, "w") as f:
            json.dump(history, f, indent=4)
            
        return {"status": "saved", "count": len(history)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

class DeleteHistoryRequest(BaseModel):
    timestamp: float = None
    delete_all: bool = False

@app.post("/delete_history")
def delete_history(req: DeleteHistoryRequest):
    try:
        if not os.path.exists(HISTORY_FILE):
             return {"status": "empty"}
             
        with open(HISTORY_FILE, "r") as f:
            history = json.load(f)
            
        if req.delete_all:
            history = []
        elif req.timestamp:
            # Filter out the item with matching timestamp
            history = [h for h in history if h.get("timestamp") != req.timestamp]
            
        with open(HISTORY_FILE, "w") as f:
            json.dump(history, f, indent=4)
            
        return {"status": "deleted", "remaining": len(history)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/get_history")
def get_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, "r") as f:
            return json.load(f)
    return []

@app.post("/notify_telegram")
def notify_telegram(req: NotifyRequest):
    """
    Send a message to the Telegram Bot.
    Uses 'TG_BOT_TOKEN' and 'TG_CHAT_ID' (optional default) or just sends updates.
    """
    token = os.getenv("TG_BOT_TOKEN")
    chat_id = os.getenv("TG_CHAT_ID", "194014207") # Default from memory
    
    if not token:
         raise HTTPException(status_code=500, detail="TG_BOT_TOKEN not set")
         
    url = f"https://api.telegram.org/bot{token}/sendMessage"
    payload = {"chat_id": chat_id, "text": req.message}
    
    try:
        import requests
        res = requests.post(url, json=payload)
        if res.status_code != 200:
             raise HTTPException(status_code=res.status_code, detail=res.text)
        return {"status": "sent"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
